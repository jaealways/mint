{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from data_transformation.db_env import DbEnv, db\n",
    "\n",
    "# Native libraries\n",
    "import os\n",
    "import math\n",
    "# Essential Libraries\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Algorithms\n",
    "from minisom import MiniSom\n",
    "from tslearn.barycenters import dtw_barycenter_averaging\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/izzettunc/introduction-to-time-series-clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 106, 107, 108, 110, 111, 112, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 153, 154, 155, 156, 157, 158, 159, 160, 161, 163, 164, 165, 167, 168, 169, 171, 173, 174, 175, 176, 177, 178, 179, 180, 183, 184, 185, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197, 198, 199, 202, 204, 206, 207, 208, 211, 212, 216, 218, 219, 221, 224, 225, 230, 231, 233, 237, 238, 240, 258, 261, 262, 265, 267, 274, 311, 322, 345, 373, 382, 420, 460, 461, 462, 464, 465, 466, 467, 468, 469, 470, 471, 472, 474, 475, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 509, 511, 512, 513, 514, 515, 516, 517, 519, 520, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 553, 554, 555, 556, 557, 558, 559, 560, 561, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 618, 620, 621, 622, 623, 624, 625, 630, 631, 632, 637, 638, 640, 642, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 659, 661, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 695, 696, 697, 698, 699, 700, 701, 702, 703, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 719, 721, 722, 723, 725, 727, 729, 730, 731, 733, 734, 735, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 781, 782, 783, 785, 789, 800, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 829, 831, 833, 835, 836, 838, 839, 840, 841, 842, 844, 845, 846, 847, 849, 850, 851, 853, 854, 855, 857, 858, 859, 861, 863, 864, 865, 866, 868, 869, 871, 872, 873, 874, 875, 877, 878, 879, 880, 881, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 909, 910, 911, 913, 914, 915, 916, 917, 918, 919, 920, 922, 923, 925, 926, 932, 934, 936, 938, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 973, 974, 976, 977, 978, 979, 980, 981, 982, 983, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 1000, 1001, 1005, 1006, 1007, 1008, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1023, 1024, 1025, 1026, 1027, 1029, 1031, 1032, 1035, 1038, 1039, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1053, 1054, 1055, 1056, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1080, 1082, 1084, 1087, 1088, 1089, 1090, 1093, 1094, 1095, 1097, 1098, 1101, 1102, 1103, 1104, 1105, 1107, 1108, 1109, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1121, 1122, 1123, 1124, 1126, 1127, 1128, 1130, 1131, 1132, 1134, 1140, 1141, 1145, 1146, 1147, 1149, 1151, 1152, 1156, 1157, 1158, 1159, 1160, 1162, 1165, 1166, 1167, 1169, 1170, 1171, 1172, 1176, 1181, 1182, 1184, 1185, 1186, 1187, 1188, 1191, 1192, 1193, 1202, 1208, 1221, 1230, 1238, 1259, 1285, 1318, 1331, 1343, 1344, 1346, 1348, 1350, 1351, 1353, 1356, 1367, 1368, 1369, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1386, 1387, 1427, 1388, 1420, 1421, 1422, 1423, 1428, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1539, 1540, 1541, 1542, 1545, 1546, 1550, 1551, 1556, 1558, 1559]\n"
     ]
    }
   ],
   "source": [
    "#get song_num list\n",
    "# 지수 추가해서 그래프에서 비교 가능하게 해주기, 클러스터링 기준?\n",
    "\n",
    "conn, cursor = DbEnv().connect_sql()\n",
    "sql = \"SELECT DISTINCT num FROM daily_music_cow\"\n",
    "num_list = DbEnv().get_data_from_table(cursor, sql)\n",
    "num_list = [item[0] for item in num_list]\n",
    "print(num_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               26     27     28     29     30     31     32     33     34  \\\n",
      "date                                                                        \n",
      "2021-06-03  29000  26000  24000  52800  13500  16100  10300  14500  36400   \n",
      "2021-06-04  29000  26000  24000  52800  13500  16100  10300  14500  36400   \n",
      "2021-06-05  29000  26000  23700  52800  13500  16100  10800  14500  36400   \n",
      "2021-06-06  28500  26000  23500  52800  13500  16100  10800  14500  36400   \n",
      "2021-06-07  29100  26000  24900  52800  13500  15100  10800  14500  36400   \n",
      "...           ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
      "2021-11-23  29000  43000  29800  90700  19100  70100  15100  37200  40000   \n",
      "2021-11-24  28500  43000  29900  90700  19100  70000  15000  37200  40000   \n",
      "2021-11-25  27900  43000  29800  90700  19100  70000  15000  37200  40000   \n",
      "2021-11-26  30000  43000  31200  90700  19100  70000  15000  37200  40000   \n",
      "2021-11-27  27000  43000  29100  90700  19000  70000  16100  37200  40000   \n",
      "\n",
      "               35  ...     1540     1541     1542     1545     1546     1550  \\\n",
      "date               ...                                                         \n",
      "2021-06-03  14300  ...      NaN      NaN      NaN      NaN      NaN      NaN   \n",
      "2021-06-04  14300  ...      NaN      NaN      NaN      NaN      NaN      NaN   \n",
      "2021-06-05  14300  ...      NaN      NaN      NaN      NaN      NaN      NaN   \n",
      "2021-06-06  14300  ...      NaN      NaN      NaN      NaN      NaN      NaN   \n",
      "2021-06-07  14300  ...      NaN      NaN      NaN      NaN      NaN      NaN   \n",
      "...           ...  ...      ...      ...      ...      ...      ...      ...   \n",
      "2021-11-23  40000  ...  22800.0  19000.0  27600.0  34000.0  27500.0  17600.0   \n",
      "2021-11-24  40000  ...  23000.0  18500.0  27600.0  33900.0  26100.0  14100.0   \n",
      "2021-11-25  40000  ...  22800.0  17500.0  27600.0  34000.0  25500.0  14200.0   \n",
      "2021-11-26  21500  ...  22300.0  17500.0  27000.0  32100.0  25000.0  15300.0   \n",
      "2021-11-27  67900  ...  22400.0  18900.0  24300.0  35200.0  25000.0  14400.0   \n",
      "\n",
      "               1551     1556     1558     1559  \n",
      "date                                            \n",
      "2021-06-03      NaN      NaN      NaN      NaN  \n",
      "2021-06-04      NaN      NaN      NaN      NaN  \n",
      "2021-06-05      NaN      NaN      NaN      NaN  \n",
      "2021-06-06      NaN      NaN      NaN      NaN  \n",
      "2021-06-07      NaN      NaN      NaN      NaN  \n",
      "...             ...      ...      ...      ...  \n",
      "2021-11-23  18100.0  45900.0  15000.0  14900.0  \n",
      "2021-11-24  17600.0  44600.0  14300.0  17400.0  \n",
      "2021-11-25  17600.0  44600.0  15600.0  15000.0  \n",
      "2021-11-26  19200.0  44600.0  15600.0  15000.0  \n",
      "2021-11-27  19200.0  44600.0  15600.0  14900.0  \n",
      "\n",
      "[178 rows x 814 columns]\n"
     ]
    }
   ],
   "source": [
    "df_price, df_mcpi = pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "# find shortest date length to fit all songs\n",
    "for num in num_list:\n",
    "    sql = \"SELECT date, price FROM daily_music_cow WHERE num = %s\" % num\n",
    "    df_temp = db(cursor, sql).dataframe\n",
    "    df_temp = df_temp.set_index('date')\n",
    "    df_temp.columns = [\"%d\" % num]\n",
    "\n",
    "    df_price = pd.concat([df_price, df_temp], axis=1)\n",
    "\n",
    "print(df_price)\n",
    "df_price.to_pickle(\"../storage/df_price.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 0\n",
      "date              \n",
      "2021-12-08  246.43\n",
      "2021-12-07  247.01\n",
      "2021-12-06  246.48\n",
      "2021-12-05  246.18\n",
      "2021-12-04  245.84\n",
      "...            ...\n",
      "2019-03-24   83.54\n",
      "2019-03-23   83.59\n",
      "2019-03-22   83.44\n",
      "2019-03-21   82.88\n",
      "2019-03-20   82.59\n",
      "\n",
      "[1990 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "sql = \"SELECT date, price FROM daily_mcpi\"\n",
    "df_mcpi = db(cursor, sql).dataframe\n",
    "df_mcpi = df_mcpi.set_index('date')\n",
    "df_mcpi.columns = [0]\n",
    "\n",
    "print(df_mcpi)\n",
    "df_mcpi.to_pickle(\"../storage/df_mcpi.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidIndexError",
     "evalue": "Reindexing only valid with uniquely valued Index objects",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidIndexError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-573752b4308d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf_mcpi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_mcpi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mdf_price\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_mcpi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_price\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mdf_price_droped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_price\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    296\u001b[0m     )\n\u001b[0;32m    297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 298\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    514\u001b[0m                     \u001b[0mobj_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnew_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mequals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 516\u001b[1;33m                         \u001b[0mindexers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    517\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_indexer\u001b[1;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[0;32m   3164\u001b[0m             \u001b[0mthis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3165\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3166\u001b[1;33m             return this.get_indexer(\n\u001b[0m\u001b[0;32m   3167\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3168\u001b[0m             )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_indexer\u001b[1;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[0;32m   3169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3170\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3171\u001b[1;33m             raise InvalidIndexError(\n\u001b[0m\u001b[0;32m   3172\u001b[0m                 \u001b[1;34m\"Reindexing only valid with uniquely valued Index objects\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3173\u001b[0m             )\n",
      "\u001b[1;31mInvalidIndexError\u001b[0m: Reindexing only valid with uniquely valued Index objects"
     ]
    }
   ],
   "source": [
    "# 결측치 제거를 통해 date 맞춰주기\n",
    "df_price = pd.read_pickle('../storage/df_price.pkl')\n",
    "df_mcpi = pd.read_pickle('../storage/df_mcpi.pkl')\n",
    "\n",
    "df_price = pd.concat([df_mcpi, df_price], axis=1)\n",
    "\n",
    "df_price_droped = df_price.dropna(axis=0)\n",
    "df_price_droped = df_price_droped.sort_index(ascending=True)\n",
    "list_price_droped = list(df_price_droped.columns)\n",
    "\n",
    "print(df_price_droped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinMaxScaler로 정규화\n",
    "array_price_scaled = MinMaxScaler().fit_transform(df_price_droped)\n",
    "df_price_scaled = pd.DataFrame(array_price_scaled)\n",
    "df_price_scaled.columns = list_price_droped\n",
    "df_price_scaled.index = df_price_droped.index\n",
    "print(df_price_scaled)\n",
    "\n",
    "# num=0(MCPI) 지수 df에서 제거\n",
    "df_mcpi = df_price_scaled[df_price_scaled.columns[0]]\n",
    "df_price_scaled = df_price_scaled.drop(df_price_scaled.columns[0], axis='columns')\n",
    "array_price_scaled = np.delete(array_price_scaled, 0, 1)\n",
    "\n",
    "print(df_mcpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_price_scaled = np.transpose(array_price_scaled)\n",
    "\n",
    "print(df_mcpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) SOM raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "som_x = som_y = math.ceil(math.sqrt(math.sqrt(len(array_price_scaled))))\n",
    "\n",
    "\n",
    "som = MiniSom(som_x, som_y,len(array_price_scaled[0]), sigma=0.3, learning_rate=0.1)\n",
    "\n",
    "som.random_weights_init(array_price_scaled)\n",
    "som.train(array_price_scaled, 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_som_series_averaged_center(som_x, som_y, win_map):\n",
    "    fig, axs = plt.subplots(som_x,som_y,figsize=(25,25))\n",
    "    fig.suptitle('Clusters')\n",
    "    for x in range(som_x):\n",
    "        for y in range(som_y):\n",
    "            cluster = (x,y)\n",
    "            if cluster in win_map.keys():\n",
    "                for series in win_map[cluster]:\n",
    "                    axs[cluster].plot(series,c=\"gray\",alpha=0.5) \n",
    "                axs[cluster].plot(df_mcpi, c=\"red\")\n",
    "                axs[cluster].plot(np.average(np.vstack(win_map[cluster]),axis=0),c=\"blue\")\n",
    "            cluster_number = x*som_y+y+1\n",
    "            axs[cluster].set_title(f\"Cluster {cluster_number}\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_map = som.win_map(array_price_scaled)\n",
    "\n",
    "plot_som_series_averaged_center(som_x, som_y, win_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_c = []\n",
    "cluster_n = []\n",
    "for x in range(som_x):\n",
    "    for y in range(som_y):\n",
    "        cluster = (x,y)\n",
    "        if cluster in win_map.keys():\n",
    "            cluster_c.append(len(win_map[cluster]))\n",
    "        else:\n",
    "            cluster_c.append(0)\n",
    "        cluster_number = x*som_y+y+1\n",
    "        cluster_n.append(f\"Cluster {cluster_number}\")\n",
    "\n",
    "plt.figure(figsize=(25,5))\n",
    "plt.title(\"Cluster Distribution for SOM\")\n",
    "plt.bar(cluster_n,cluster_c)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>시계열이 시각적으로도 잘 구분을 못함<br/>\n",
    "<br>위아래로의 등락이 너무 Chaotic하기 때문이라고 판단.<br/>\n",
    "<br>특히 뮤직카우의 경우 유동성이 충분하지 않기 때문에 적절한 균형가격에 Optimize되지 않는 문제<br/>\n",
    "<br>Smoothing을 통해 다시 분류<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # (2) SOM holtwinters weight 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#smoothing 0.3\n",
    "array_price = df_price_droped.to_numpy()\n",
    "array_price = np.transpose(array_price)\n",
    "\n",
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing, Holt\n",
    "\n",
    "array_price_smooth = np.empty((0, 145))\n",
    "\n",
    "for x in array_price:\n",
    "    model_smooth = SimpleExpSmoothing(x)\n",
    "    fit_smooth = model_smooth.fit(smoothing_level=.3)\n",
    "    array_price_smooth = np.append(array_price_smooth, fit_smooth.fittedvalues.reshape(1, 145), axis=0)\n",
    "\n",
    "print(array_price_smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinMaxScaler로 정규화\n",
    "array_price_smooth = np.transpose(array_price_smooth)\n",
    "array_price_smooth_scaled = MinMaxScaler().fit_transform(array_price_smooth)\n",
    "df_price_smooth_scaled = pd.DataFrame(array_price_smooth_scaled)\n",
    "df_price_smooth_scaled.columns = list_price_droped\n",
    "df_price_smooth_scaled.index = df_price_droped.index\n",
    "print(df_price_smooth_scaled)\n",
    "\n",
    "# num=0(MCPI) 지수 df에서 제거\n",
    "df_mcpi = df_price_smooth_scaled[df_price_smooth_scaled.columns[0]]\n",
    "df_price_scaled = df_price_smooth_scaled.drop(df_price_smooth_scaled.columns[0], axis='columns')\n",
    "array_price_smooth_scaled = np.delete(array_price_smooth_scaled, 0, 1)\n",
    "\n",
    "print(df_mcpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_price_smooth_scaled = np.transpose(array_price_smooth_scaled)\n",
    "\n",
    "som_x = som_y = math.ceil(math.sqrt(math.sqrt(len(array_price_smooth_scaled))))\n",
    "\n",
    "\n",
    "som = MiniSom(som_x, som_y,len(array_price_smooth_scaled[0]), sigma=0.3, learning_rate=0.1)\n",
    "\n",
    "som.random_weights_init(array_price_smooth_scaled)\n",
    "som.train(array_price_smooth_scaled, 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "win_map = som.win_map(array_price_smooth_scaled)\n",
    "\n",
    "plot_som_series_averaged_center(som_x, som_y, win_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) SOM holtwinters weight 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#smoothing 0.5\n",
    "array_price = df_price_droped.to_numpy()\n",
    "array_price = np.transpose(array_price)\n",
    "\n",
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing, Holt\n",
    "\n",
    "array_price_smooth = np.empty((0, 145))\n",
    "\n",
    "for x in array_price:\n",
    "    model_smooth = SimpleExpSmoothing(x)\n",
    "    fit_smooth = model_smooth.fit(smoothing_level=.5)\n",
    "    array_price_smooth = np.append(array_price_smooth, fit_smooth.fittedvalues.reshape(1, 145), axis=0)\n",
    "print(array_price_smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinMaxScaler로 정규화\n",
    "array_price_smooth = np.transpose(array_price_smooth)\n",
    "array_price_smooth_scaled = MinMaxScaler().fit_transform(array_price_smooth)\n",
    "df_price_smooth_scaled = pd.DataFrame(array_price_smooth_scaled)\n",
    "df_price_smooth_scaled.columns = list_price_droped\n",
    "df_price_smooth_scaled.index = df_price_droped.index\n",
    "print(df_price_smooth_scaled)\n",
    "\n",
    "# num=0(MCPI) 지수 df에서 제거\n",
    "df_mcpi = df_price_smooth_scaled[df_price_smooth_scaled.columns[0]]\n",
    "df_price_scaled = df_price_smooth_scaled.drop(df_price_smooth_scaled.columns[0], axis='columns')\n",
    "array_price_smooth_scaled = np.delete(array_price_smooth_scaled, 0, 1)\n",
    "\n",
    "\n",
    "print(df_mcpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_price_smooth_scaled = np.transpose(array_price_smooth_scaled)\n",
    "\n",
    "som_x = som_y = math.ceil(math.sqrt(math.sqrt(len(array_price_smooth_scaled))))\n",
    "\n",
    "\n",
    "som = MiniSom(som_x, som_y,len(array_price_smooth_scaled[0]), sigma=0.3, learning_rate=0.1)\n",
    "\n",
    "som.random_weights_init(array_price_smooth_scaled)\n",
    "som.train(array_price_smooth_scaled, 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_map = som.win_map(array_price_smooth_scaled)\n",
    "\n",
    "plot_som_series_averaged_center(som_x, som_y, win_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_map = []\n",
    "for idx in range(len(array_price_smooth_scaled)):\n",
    "    winner_node = som.winner(array_price_smooth_scaled[idx])\n",
    "    cluster_map.append((list_price_droped[idx],f\"Cluster {winner_node[0]*som_y+winner_node[1]+1}\"))\n",
    "\n",
    "df_cluster = pd.DataFrame(cluster_map,columns=[\"Series\",\"Cluster\"]).sort_values(by=\"Cluster\").set_index(\"Series\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_c = []\n",
    "cluster_n = []\n",
    "for x in range(som_x):\n",
    "    for y in range(som_y):\n",
    "        cluster = (x,y)\n",
    "        if cluster in win_map.keys():\n",
    "            cluster_c.append(len(win_map[cluster]))\n",
    "        else:\n",
    "            cluster_c.append(0)\n",
    "        cluster_number = x*som_y+y+1\n",
    "        cluster_n.append(f\"Cluster {cluster_number}\")\n",
    "\n",
    "plt.figure(figsize=(25,5))\n",
    "plt.title(\"Cluster Distribution for SOM\")\n",
    "plt.bar(cluster_n,cluster_c)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "시계열에서 MCPI 효과 제거할 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = DbEnv().connect_mongo('music_cow', 'daily_music_cow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_cluster = df_cluster[df_cluster.Cluster == \"Cluster 1\"].index.tolist()\n",
    "print(list_cluster)\n",
    "for cl in list_cluster:\n",
    "    list_song = conn.find({'num': int(cl)})\n",
    "    for x in list_song:\n",
    "        print('title:', x['song_title'], '---', 'artist:', x['song_artist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_cluster = df_cluster[df_cluster.Cluster == \"Cluster 2\"].index.tolist()\n",
    "print(list_cluster)\n",
    "for cl in list_cluster:\n",
    "    list_song = conn.find({'num': int(cl)})\n",
    "    for x in list_song:\n",
    "        print('title:', x['song_title'], '---', 'artist:', x['song_artist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_cluster = df_cluster[df_cluster.Cluster == \"Cluster 3\"].index.tolist()\n",
    "print(list_cluster)\n",
    "for cl in list_cluster:\n",
    "    list_song = conn.find({'num': int(cl)})\n",
    "    for x in list_song:\n",
    "        print('title:', x['song_title'], '---', 'artist:', x['song_artist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_cluster = df_cluster[df_cluster.Cluster == \"Cluster 4\"].index.tolist()\n",
    "print(list_cluster)\n",
    "for cl in list_cluster:\n",
    "    list_song = conn.find({'num': int(cl)})\n",
    "    for x in list_song:\n",
    "        print('title:', x['song_title'], '---', 'artist:', x['song_artist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_cluster = df_cluster[df_cluster.Cluster == \"Cluster 5\"].index.tolist()\n",
    "print(list_cluster)\n",
    "for cl in list_cluster:\n",
    "    list_song = conn.find({'num': int(cl)})\n",
    "    for x in list_song:\n",
    "        print('title:', x['song_title'], '---', 'artist:', x['song_artist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_cluster = df_cluster[df_cluster.Cluster == \"Cluster 6\"].index.tolist()\n",
    "print(list_cluster)\n",
    "for cl in list_cluster:\n",
    "    list_song = conn.find({'num': int(cl)})\n",
    "    for x in list_song:\n",
    "        print('title:', x['song_title'], '---', 'artist:', x['song_artist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_cluster = df_cluster[df_cluster.Cluster == \"Cluster 7\"].index.tolist()\n",
    "print(list_cluster)\n",
    "for cl in list_cluster:\n",
    "    list_song = conn.find({'num': int(cl)})\n",
    "    for x in list_song:\n",
    "        print('title:', x['song_title'], '---', 'artist:', x['song_artist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_cluster = df_cluster[df_cluster.Cluster == \"Cluster 8\"].index.tolist()\n",
    "print(list_cluster)\n",
    "for cl in list_cluster:\n",
    "    list_song = conn.find({'num': int(cl)})\n",
    "    for x in list_song:\n",
    "        print('title:', x['song_title'], '---', 'artist:', x['song_artist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_cluster = df_cluster[df_cluster.Cluster == \"Cluster 9\"].index.tolist()\n",
    "print(list_cluster)\n",
    "for cl in list_cluster:\n",
    "    list_song = conn.find({'num': int(cl)})\n",
    "    for x in list_song:\n",
    "        print('title:', x['song_title'], '---', 'artist:', x['song_artist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_cluster = df_cluster[df_cluster.Cluster == \"Cluster 10\"].index.tolist()\n",
    "print(list_cluster)\n",
    "for cl in list_cluster:\n",
    "    list_song = conn.find({'num': int(cl)})\n",
    "    for x in list_song:\n",
    "        print('title:', x['song_title'], '---', 'artist:', x['song_artist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_cluster = df_cluster[df_cluster.Cluster == \"Cluster 11\"].index.tolist()\n",
    "print(list_cluster)\n",
    "for cl in list_cluster:\n",
    "    list_song = conn.find({'num': int(cl)})\n",
    "    for x in list_song:\n",
    "        print('title:', x['song_title'], '---', 'artist:', x['song_artist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_cluster = df_cluster[df_cluster.Cluster == \"Cluster 12\"].index.tolist()\n",
    "print(list_cluster)\n",
    "for cl in list_cluster:\n",
    "    list_song = conn.find({'num': int(cl)})\n",
    "    for x in list_song:\n",
    "        print('title:', x['song_title'], '---', 'artist:', x['song_artist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_cluster = df_cluster[df_cluster.Cluster == \"Cluster 13\"].index.tolist()\n",
    "print(list_cluster)\n",
    "for cl in list_cluster:\n",
    "    list_song = conn.find({'num': int(cl)})\n",
    "    for x in list_song:\n",
    "        print('title:', x['song_title'], '---', 'artist:', x['song_artist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_cluster = df_cluster[df_cluster.Cluster == \"Cluster 14\"].index.tolist()\n",
    "print(list_cluster)\n",
    "for cl in list_cluster:\n",
    "    list_song = conn.find({'num': int(cl)})\n",
    "    for x in list_song:\n",
    "        print('title:', x['song_title'], '---', 'artist:', x['song_artist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_cluster = df_cluster[df_cluster.Cluster == \"Cluster 15\"].index.tolist()\n",
    "print(list_cluster)\n",
    "for cl in list_cluster:\n",
    "    list_song = conn.find({'num': int(cl)})\n",
    "    for x in list_song:\n",
    "        print('title:', x['song_title'], '---', 'artist:', x['song_artist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_cluster = df_cluster[df_cluster.Cluster == \"Cluster 16\"].index.tolist()\n",
    "print(list_cluster)\n",
    "for cl in list_cluster:\n",
    "    list_song = conn.find({'num': int(cl)})\n",
    "    for x in list_song:\n",
    "        print('title:', x['song_title'], '---', 'artist:', x['song_artist'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지수효과 제거해줄 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dynamic Time Warping Barycenter Averaging (DBA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from scipy import interpolate\n",
    "import numpy as np\n",
    "from dtaidistance import dtw\n",
    "import matplotlib.pyplot as plt\n",
    "from _plotly_future_ import v4_subplots\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OF_TRAJECTORIES = 200\n",
    "MIN_LEN_OF_TRAJECTORY = 16\n",
    "MAX_LEN_OF_TRAJECTORY = 40\n",
    "THRESHOLD = 0.50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/time-series-hierarchical-clustering-using-dynamic-time-warping-in-python-c8c9edf2fda5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_price_scaled = deepcopy(array_price_scaled)\n",
    "distanceMatrixDictionary = {}\n",
    "iteration = 1\n",
    "while True:\n",
    "   distanceMatrix = np.empty((len(array_price_scaled), len(array_price_scaled),))\n",
    "   distanceMatrix[:] = np.nan\n",
    "   \n",
    "   for index1, (filter1, trajectory1) in enumerate(trajectories.items()):\n",
    "      tempArray = []\n",
    "      \n",
    "      for index2, (filter2, trajectory2) in enumerate(trajectories.items()):\n",
    "         \n",
    "         if index1 > index2:\n",
    "            continue\n",
    "         \n",
    "         elif index1 == index2:\n",
    "            continue\n",
    "         \n",
    "         else:\n",
    "            unionFilter = filter1 + filter2\n",
    "            sorted(unionFilter)\n",
    "            \n",
    "            if unionFilter in distanceMatrixDictionary.keys():\n",
    "               distanceMatrix[index1][index2] = distanceMatrixDictionary.get(unionFilter)\n",
    "               \n",
    "               continue\n",
    "            \n",
    "            metric = []\n",
    "            for subItem1 in trajectory1:\n",
    "               \n",
    "               for subItem2 in trajectory2:\n",
    "                  metric.append(dtw.distance(subItem1, subItem2, psi=1))\n",
    "            \n",
    "            metric = max(metric)\n",
    "            \n",
    "            distanceMatrix[index1][index2] = metric\n",
    "            distanceMatrixDictionary[unionFilter] = metric"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
